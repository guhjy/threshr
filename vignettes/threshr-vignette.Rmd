---
title: "Introducing threshr: Threshold Selection and Uncertainty for Extreme Value Analysis"
author: "Paul J. Northrop"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introducing threshr: Threshold Selection and Uncertainty for Extreme Value Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: threshr.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(comment = "#>", collapse = TRUE)
```

The *threshr* package deals primarily with the selection of thresholds for use in extreme value modelling.  The underlying methodology is described in detail in [Northrop, Attalides, and Jonathan (2017)](https://doi.org/10.1111/rssc.12159). Bayesian leave-one-out cross-validation is used to compare the extreme value predictive abilities resulting from each of a set of thresholds.  This assesses the trade-off between the model mis-specification bias that results from an inappropriately low threshold and the loss of precision of estimation from an unnecessarily high threshold. There many other approaches to address this bias-variance trade-off. See @SM2012 for a review.

At the moment only the simplest case, where the data can be treated as independent identically distributed observations, is considered.  In this case the model used is a combination of a binomial distribution for the number of *exceedances* of a given threshold and a generalized Pareto (GP) distribution for the amounts, the *threshold excesses* by which exceedances lie above a threshold.  We refer to this as a binomial-GP model. Future releases will tackle more general situations.  

We use the function `ithresh` to compare the predictive performances of each of a set of user-supplied thresholds.  We also perform predictive inferences for future extreme valus, using the `predict` method for objects returned from `ithresh`.  These inferences can be based either on a single threshold or on a weighted average of inferences from multiple thresholds.  The weighting reflects an estimated measure of the predictive performance of the threshold and can also incorporate use-supplied prior probabilities for eac threshold.  

A traditional simple graphical method to inform threshold selection is to plot estimates of, and confidence intervals for, the GP shape parameter $\xi$ over a range of thresholds. This plot is used to choose a threshold above which the underlying GP shape parameter may be approximately constant. See Chapter 4 of see @Coles2001 for details. Identifying a single threshold using this method is usually unrealistic but the plot can point to a range of thresholds that merit more sophisticated analysis. The **threshr** function `stability` produces this type of plot.

## Cross-validatory predictive performance for i.i.d. data using `ithresh`

We provide a brief outline of the methodology underlying `ithresh`.  For full details see @NAJ2017. We set a vector of *training thresholds* $u_1, \ldots, u_k$. A *validation threshold* $v = u_k$ defines validation data: indicators of whether or not an observation exceeds $v$ and, if it does, the amount by which $v$ is exceeded.  For a given training threshold leave-one-out cross-validation estimates the quality of predictive inference for the individual omitted samples based on Bayesian inferences from a binomial-GP model. Importance sampling is used to reduce computation time: only two posterior samples are required for each training threshold.  Simulation from the posterior distributions of the binomial-GP parameters is performed using the **revdbayes** package [@revdbayes].

In the first release the binomial probability is assumed to be independent of the parameters of the GP distribution *a priori*.  This will be relaxed in a later release.  The user can choose from a selection of in-built prior distributions and may specify their own prior for GP models parameters.  By default the Beta(1/2, 1/2) Jeffreys' prior is used for the threshold exceedance probability of the binomial distribution and a prior related to the Maximal Data Information (MDI) prior is used for the GP parameters.  See @NAJ2017 for details of the latter.

We use the datasets `gom` and `ns` to illustrate the code.  These are the same datasets analysed in @NAJ2017.

```{r, include = FALSE}
set.seed(22082017)
```

```{r, fig.show='hold', fig.height=5, fig.width=7, fig.align='center'}
library(threshr)
# Set the size of the posterior sample simulated at each threshold
n <- 1000
# North Sea significant wave heights
ns_u_vec <- quantile(ns, probs = seq(0, 0.85, by = 0.05))
ns_cv <- ithresh(data = ns, u_vec = ns_u_vec, n = n)
```

```{r, include = FALSE}
set.seed(2208217)
```

```{r}
# Gulf of Mexico significant wave heights
gom_u_vec <- quantile(gom, probs = seq(0, 0.8, by = 0.05))
gom_cv <- ithresh(data = gom, u_vec = gom_u_vec, n = n)
```

```{r}
sum_ns <- summary(ns_cv)
sum_ns
u_ns <- sum_ns[1, 5]
u_ns

sum_gom <- summary(gom_cv)
sum_gom
u_gom <- sum_gom[1, 5]
u_gom
```

We produce plots like those in the top row of Figure 7 in @NAJ2017.

```{r, fig.show='hold', fig.height=5, fig.width=7, fig.align='center'}
plot(ns_cv, lwd = 2, cex.axis = 0.8)
mtext("significant wave height / m", side = 3, line = 2.5)

plot(gom_cv, lwd = 2, cex.axis = 0.8)
mtext("significant wave height / m", side = 3, line = 2.5)
```

```{r, fig.height=5, fig.width=5, fig.align='center'}
# Plot of Generalized Pareto posterior sample at the best threshold
# (based on the lowest validation threshold)
plot(gom_cv, which_u = "best")
# See which threshold was used
summary(gom_cv)
```

## Predictive inference for future extremes using `predict`

Let $M_N$ denote the largest value to be observed in a time period of length $N$ years.

### Single training threshold

Default: best. Use can pick their own.

```{r, fig.show='hold', fig.height=5, fig.width=7, fig.align='center'}
# Predictive distribution function
best_p <- predict(gom_cv, n_years = c(100, 1000), type = "d")
plot(best_p)
```

### Inferences averaged over multiple thresholds

User can specify a prior probability for each threshold.  The default is that all thresholds receive equal prior probability.  In this case the weights applied to individual thresholds are those displayed in the threshold diagnostic plot above.

```{r, fig.show='hold', fig.height=5, fig.width=7, fig.align='center'}
### All thresholds plus weighted average of inferences over all thresholds
all_p <- predict(gom_cv, which_u = "all")
plot(all_p)
```

As we expect, the estimated distribution function obtained by the weighted average over all thresholds lies between the extremes of the curves of the individual thresholds.

## References

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({  "HTML-CSS": { minScaleAdjust: 125, availableFonts: [] }  });
</script>
